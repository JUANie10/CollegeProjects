{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Assignment 5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDo6Mz1MoJxR",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 5\n",
        "\n",
        "Turn in the assignment via Canvas.\n",
        "\n",
        "To write legible answers you will need to be familiar with both [Markdown](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet) and [Latex](https://www.latex-tutorial.com/tutorials/amsmath/)\n",
        "\n",
        "Before you turn this problem in, make sure everything runs as expected. First, restart the kernel (in the menubar, select Runtime→→Restart runtime) and then run all cells (in the menubar, select Runtime→→Run All).\n",
        "\n",
        "Make sure you fill in any place that says \"YOUR CODE HERE\" or \"YOUR ANSWER HERE\", as well as your name below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOfazHDtoJxS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NAME = \"Juan Lee\"\n",
        "STUDENT_ID = \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "5de1d899a516090395f300c2ecc737f0",
          "grade": false,
          "grade_id": "cell-f12bff039a5016a3",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "6q2qG3e3BwDf",
        "colab_type": "text"
      },
      "source": [
        "# Question 1\n",
        "\n",
        "Using MDP, model the spread of COVID-19 for creating a simulator, under certain assumptions in a hypothetical situation. In the simulation, the inhabitants of a town move around as they go about their regular business. (The simulation would be similar to the one in [this link](https://www.washingtonpost.com/graphics/2020/world/corona-simulator/) if you are curious. However, the simulation itself is unrelated to modeling the MDP in this problem, and you can ignore it.)\n",
        "\n",
        "In the simulation, there are 2 types of people in the town, a **carrier** or **non-carrier** of the Coronavirus. The virus spreads via droplets when an infected person comes in close contact (within 6 feet) of another person who has the virus, a carrier. Non-carriers cannot transmit the disease. Associated with these two types are 5 different states that a person can be in. A carrier can be in one of the following 2 states: infected (but not sick i.e. asymptomatic), or sick. A non-carrier can be in 3 states: unexposed, dead, or immune.\n",
        "\n",
        "The states (with a distinct color representing the state in the MDP) are described as the following:\n",
        "\n",
        "\n",
        "*   **Unexposed** (BLUE): The people who haven't encountered the virus at all so far.\n",
        "*   **Infected** (ORANGE): People who have the virus but have no symptoms of the disease. An unexposed person has an 80% chance of getting infected on coming in contact with a person carrying the virus. An infected person may get sick with symptoms in 5 days or stay infected (contagious) for 15 days and develop immunity to the disease after that.\n",
        "*   **Sick** (RED): People who have the virus and are sick with symptoms. Of those infected, 50% get sick with the disease. In 10 days, a sick person may recover completely (98%), develop lifelong immunity, or die (2%).\n",
        "*   **Dead** (GRAY): People who die from the disease. 2% of the sick die.\n",
        "*   **Immune** (GREEN): There are two ways of getting to this state: 1) People who got sick with COVID-19, and recovered and thereby, have developed lifelong immunity, 2) People who were infected but didn't develop symptoms and became immune once they stopped being contagious.\n",
        "\n",
        "\n",
        "Lets assume that, the cost for running 1 day of simulation is 1 standard processing cycle of a specific computer. On day 1 of the simulation, a (random) person returned to the town from elsewhere and is sick with the virus. Unexposed people who come in contact with this carrier as they travel between home and work may become infected with the virus. There is an 80% chance of contracting the virus (and changing state to infected) on contact with a carrier. We assume that this is the only means by which an unexposed person may get infected by the virus, and they are infected immediately. To summarize, the following transitions between states are possible:\n",
        "\n",
        "*   Unexposed → Infected (happens immediately on the same day after each contact with 80% chance)\n",
        "*   Infected → Sick (after 5 days of getting infected with 50% chance)\n",
        "*   Infected → Immune (after 15 days of getting infected with 50% chance)\n",
        "*   Sick → Immune (after 10 days of getting sick with 98% chance)\n",
        "*   Sick → Dead (after 10 days of getting sick with 2% chance)\n",
        "\n",
        "a) Draw the MDP graphically\n",
        "  - A good way to do this is through [Google Drawings](https://docs.google.com/drawings)\n",
        "  - When you're done you can embed it in the jupyter notebook using markdown syntax \\!\\[alt-text\\]\\(url\\)\n",
        "  - To get the URL for your image in Google Draw goto File->Publish to the web...->Embed and copy the src portion of the html tag\n",
        "\n",
        "b) Create the transition probabilities matrix for the MDP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4IXl8khyAHk",
        "colab_type": "text"
      },
      "source": [
        "[YOUR ANSWER HERE]\n",
        "\n",
        "a)  ![alt-text](<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vQ4X_qUjdZr5VDHps2tjQ4C8hZLDfFtcStsCaEdQ0BHQXk3_FPHguF4zSSd3Rl3J0Khyr645mfjBccp/pub?w=960&amp;h=720\">\n",
        ")\n",
        "\n",
        "The cost for running 1 day of simulation is 1 standard processing cycle of a specific computer.\n",
        "\n",
        "b)![alt-text](<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vQqLdIBp53t2E5GEgXV4vfOoHZH4AMKn8l7c8oQ4bR0ctCykidr2zKwMnEN0E-jWVAy_O9ANgROTJnX/pub?w=960&amp;h=720\">)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4N8K_Ls1A87g",
        "colab_type": "text"
      },
      "source": [
        "# Question 2\n",
        "\n",
        "Solve the following  MDP using both value iteration and policy iteration.\n",
        "\n",
        "Neo is a freelance computer programmer, and his aim in life is to earn as much money as he can by writing more code. He has three possible states in his daily professional life as below.\n",
        "\n",
        "1.   **Productive** - He can write programming codes more efficiently.\n",
        "2.   **Exhausted** - He is too tired to think well to write efficient programming codes.\n",
        "3.   **Fit** - He is physically and mentally in a great state to write code and solve programming problems.\n",
        "\n",
        "\n",
        "When Neo is **Exhausted**, he can choose one of three actions: (1) Keep **Coding**, (2) do some physical **Exercise**, or (3) get some **Rest**.\n",
        "\n",
        "If he chooses to do more Coding, he remains in the **Exhausted** state with the certainty of getting a +20 dollars reward. If he decides to get Rest, he has 80% of moving to the next state, Productive, and a 20% chance of staying Exhausted.\n",
        "If he doesn’t want to get Rest, he may go to the gym and do some physical Exercise. This gives him a 50% chance of entering the Productive state and a 50% chance of staying Exhausted. However, he needs to pay for the gym, so this choice results in a -10 dollars reward.\n",
        "\n",
        "When Neo becomes **Productive**, he can go back to **Coding** and write programs more efficiently. From there, he has an 80% chance of getting Exhausted again with earning a +40 dollars reward, and a 20% chance of staying Productive while earning a +30 dollar reward.\n",
        "Sometimes, when he is Productive, he wants to do some physical **Exercise**. When he Exercises in this state, he enjoys it very much and gets 100% Fit physically and mentally. However, he needs to pay for it with 10 dollars.\n",
        "\n",
        "Once Neo reaches the state **Fit**, he is fully committed to earning more money by more **Coding**. Because he is in such a great state physically and mentally, he can write programs very efficiently. In this state, he earns a +100 dollars reward, and he keeps writing code until he is Exhausted again.\n",
        "\n",
        "Use MDP to find the best policy to maximize his earnings/rewards over time.\n",
        "\n",
        "1. Draw the MDP graphically\n",
        "  - A good way to do this is through [Google Drawings](https://docs.google.com/drawings)\n",
        "  - When you're done you can embed it in the jupyter notebook using markdown syntax \\!\\[alt-text\\]\\(url\\)\n",
        "  - To get the URL for your image in Google Draw goto File->Publish to the web...->Embed and copy the src portion of the html tag\n",
        "\n",
        "2. Using a discount factor of 0.86, solve the MDP using value iteration (until the values have become reasonably stable). You should start with the values set to zero. You should show both the optimal policy and the optimal values.\n",
        "\n",
        "3. Using a discount factor of 0.86, solve the MDP using policy evaluation (until you have complete convergence). You should start with the policy that always does **Coding**.\n",
        "\n",
        "4. Change the MDP in three different ways: by changing the discount factor, changing the transition probabilities for a single action from a single state, and by changing a reward for a single action at a single state. Each of these changes should be performed separately starting at the original MDP, resulting in three new MDPs (which you do not have to draw), each of which is different from the original MDP in a single way. In each case, the change should be so that the optimal policy changes, and you should state what the optimal policy becomes and give a short intuitive argument for this.\n",
        "\n",
        "**You can add more cells for Code or Text as you feel the need.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jckgTbK2qESC",
        "colab_type": "text"
      },
      "source": [
        "[YOUR ANSWER HERE. ADD MORE CELLS AS NEEDED.]\n",
        "![alt-text](<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vTY3_rVZ_EshZd_bw8UlH2qAPOWp19XrdgdY8yvmgy0UgaqiNMWBEdDUzvEMLuJSjSGXe-et3JZO8Io/pub?w=960&amp;h=720\">)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "748OScM6pxbU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "fa18e6bb-3dce-4b15-a623-2601ffb9abb0"
      },
      "source": [
        "# Write Your Code Here. Add more cells as needed.\n",
        "Opt_val = [0, 0, 0]\n",
        "Opt_pol = ['', '', '']\n",
        "case1 = False\n",
        "case2 = False\n",
        "case3 = False\n",
        "loop = True\n",
        "d = .86\n",
        "while(loop == True):\n",
        "  for i in range(3):\n",
        "    if i == 0:\n",
        "      if case1 == False:\n",
        "        actions = [0, 0, 0]\n",
        "        actions[0] = 20 + ((d) * Opt_val[0])\n",
        "        actions[1] = (.5) * (-10 + (d) * Opt_val[0]) + (.5) * (-10 + (d) * Opt_val[1])\n",
        "        actions[2] = (.2) * ((d) * Opt_val[0]) + (.8) * (d * Opt_val[1])\n",
        "        temp = max(actions) \n",
        "        if temp == actions[0]:\n",
        "          Opt_pol[0] = 'Coding' \n",
        "        elif temp == actions[1]:\n",
        "          Opt_pol[0] = 'Exercise'\n",
        "        elif temp == actions[2]:\n",
        "          Opt_pol[0] = 'Rest'\n",
        "        if abs(temp - Opt_val[0]) <= .01:\n",
        "          case1 = True\n",
        "        else: \n",
        "          Opt_val[0] = temp\n",
        "    elif i == 1:\n",
        "      if case2 == False:\n",
        "        actions = [0, 0]\n",
        "        actions[0] = (.8) *(40 + (d)*Opt_val[0]) + (.2) * (30 + (d*Opt_val[1]))\n",
        "        actions[1] = -10 + (d*Opt_val[2])\n",
        "        temp = max(actions)\n",
        "        if temp == actions[0]:\n",
        "          Opt_pol[1] = 'Coding' \n",
        "        elif temp == actions[1]:\n",
        "          Opt_pol[1] = 'Exercise'\n",
        "        \n",
        "        if abs(temp - Opt_val[1]) <= .01:\n",
        "          case2 = True\n",
        "        else:\n",
        "          Opt_val[1] = temp\n",
        "    elif i == 2:\n",
        "      if case3 == False:\n",
        "        temp = (100 + (d * Opt_val[0]))\n",
        "        Opt_pol[2] = 'Coding'\n",
        "        if abs(temp - Opt_val[2]) <= .01:\n",
        "          case3 = True\n",
        "        else: \n",
        "          Opt_val[2] = temp\n",
        "    if case1 == True and case2 == True and case3 == True:\n",
        "      loop = False\n",
        "  \n",
        "print(Opt_val)\n",
        "print(Opt_pol)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[163.7845498079534, 197.1264323200339, 240.85471283483992]\n",
            "['Rest', 'Exercise', 'Coding']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfMiOpOmWPej",
        "colab_type": "text"
      },
      "source": [
        "Using Value Iteration:\n",
        "\n",
        "For state Exhausted, the optimal value = 163.7845498079534 , the optimal policy = 'Rest'\n",
        "\n",
        "For state Productive, the optimal value = 197.1264323200339, the optimal policy = 'Exercise'\n",
        "\n",
        "For state Fit, the optimal value = 240.85471283483992 , the optimal policy = 'Coding'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OVangHqXa81",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "2dfcad33-cd6d-4057-ae11-c6171ca494c7"
      },
      "source": [
        "c = [0, 0, 0]\n",
        "e = [0, 0, 0]\n",
        "r = [0, 0, 0]\n",
        "opt_pol = ['', '', '']\n",
        "case1 = False\n",
        "case2 = False\n",
        "case3 = False\n",
        "loop = True\n",
        "while(loop == True):\n",
        "  for i in range(3):\n",
        "    if i == 0:\n",
        "      if case1 == False:\n",
        "        tempc = c[0]\n",
        "        c[0] = 20 + ((.86) * c[0])\n",
        "        tempe = e[0]\n",
        "        e[0] = (.5) * (-10 + (.86) *e[0]) + (.5) * (-10 + (.86) * e[1])\n",
        "        tempr = r[0]\n",
        "        r[0] = (.2) * ((.86) * r[0]) + (.8) * (.86 * r[1])\n",
        "        opt_pol[0] = 'Rest'\n",
        "        if abs(tempc - c[0]) <= .01 and abs(tempe - e[0]) <= .01 and abs(tempr - r[0]) <= .01:\n",
        "          case1 = True\n",
        "\n",
        "    elif i == 1:\n",
        "      if case2 == False:\n",
        "        tempc = c[1]\n",
        "        c[1] = (.8) *(40 + (.86)*c[0]) + (.2) * (30 + (.86*c[1]))\n",
        "        tempe = e[1]\n",
        "        e[1] = -10 + (.86*e[2])\n",
        "        opt_pol[1] = 'Exercise'\n",
        "        if abs(tempc - c[1]) <= .01 and abs(tempe - e[1]) <= .01:\n",
        "          case2 = True\n",
        "\n",
        "    elif i == 2:\n",
        "      if case3 == False:\n",
        "        temp = c[2]\n",
        "        c[2] = (100 + (.86 * c[0]))\n",
        "        opt_pol[2] = 'Coding'\n",
        "        if abs(temp - c[2]) <= .01:\n",
        "          case3 = True\n",
        "    if case1 == True and case2 == True and case3 == True:\n",
        "      loop = False\n",
        "\n",
        "optimal = [max([c[0]+20, e[0], r[0]]),max([c[1]+20, e[1], r[1]]), max([c[2]+20, e[2], r[2]])]\n",
        "\n",
        "\n",
        "print(optimal)\n",
        "print(opt_pol)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[162.80106112424494, 184.54019155902756, 242.80106112424494]\n",
            "['Rest', 'Exercise', 'Coding']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSCrLzDSXafV",
        "colab_type": "text"
      },
      "source": [
        "Using Policy Evaluation:\n",
        "\n",
        "For state Exhausted, the optimal value = 162.80106112424494 , the optimal policy = 'Rest'\n",
        "\n",
        "For state Productive, the optimal value = 184.54019155902756, the optimal policy = 'Exercise'\n",
        "\n",
        "For state Fit, the optimal value = 242.80106112424494 , the optimal policy = 'Coding'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN_Nyy8Wow16",
        "colab_type": "text"
      },
      "source": [
        "4. By changing the discount factor to 0, the only reward that matters is the instantaneous rewards so the optimal policy for all the states would be coding because coding grants the highest instant reward for each state. By changing transitional probabily of excersizing for exhauted state to have probability to return to exhausted be .8 and going to productive .2 changes the optimal policy to coding instead of rest because the guarentee of earning 20 rewards grants more value in the long run. Lastly changing the reward of coding for Productive state to be 100 for returning to exhausted state would make the optimal policy for productive to be coding instead of excersize because the value calculated in the long run would provide the maximum amount of reward for the state of Productive. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQJg8PvNcRGt",
        "colab_type": "text"
      },
      "source": [
        "# Question 3\n",
        "\n",
        "## Markov Decision Process (MDP) Toolbox for Python\n",
        "\n",
        "One useful Python module for solving these types of problems is called *MDP toolbox* (pymdptoolbox). The MDP toolbox provides classes and functions for the resolution of descrete-time Markov Decision Processes. The list of algorithms that have been implemented includes backwards induction, linear programming, policy iteration, q-learning and value iteration along with several variations. In the next cell you'll see how to load this module into a Jupyter notebook running in Colab. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WrBebxhcJID",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "102f0594-0f56-43d0-b747-77f62a2587d4"
      },
      "source": [
        "!pip install pymdptoolbox"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymdptoolbox in /usr/local/lib/python3.6/dist-packages (4.0b3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pymdptoolbox) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pymdptoolbox) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTl9nuj3Zhr5",
        "colab_type": "text"
      },
      "source": [
        "A simple example code is given below. For details about the example, check the documentation for Python MDP Toolbox (pymdptoolbox) from this [link](https://pymdptoolbox.readthedocs.io/en/latest/index.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vKT0UTgZik-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "c8eb003b-aeb8-42db-d157-576eefb3523f"
      },
      "source": [
        "import numpy as np\n",
        "import mdptoolbox\n",
        "import mdptoolbox.example\n",
        "np.random.seed(0) # Needed to get the output below\n",
        "P, R = mdptoolbox.example.rand(2, 2)\n",
        "\n",
        "pi = mdptoolbox.mdp.PolicyIteration(P, R, 0.9)\n",
        "pi.run()\n",
        "print(pi.policy)\n",
        "print(pi.iter)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 0)\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QW-tv3L3x99",
        "colab_type": "text"
      },
      "source": [
        "##Solving a MDP using Python MDP Toolbox\n",
        "\n",
        "First read the documentation for Python MDP Toolbox (pymdptoolbox) from this [link](https://pymdptoolbox.readthedocs.io/en/latest/index.html) and try to understand how it works.\n",
        "\n",
        "Then solve the MDP from Question 2 using both value iteration and policy iteration (it is same as policy evaluation) algorithms from the Python MDP Toolbox.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfem6k9-ceSg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "116fe5fa-6d8c-4be8-fc95-81fbf8a0bb4e"
      },
      "source": [
        "# Value Iteration.\n",
        "\n",
        "states = ['Exhuasted', 'Productive', 'Fit']\n",
        "actions = ['Coding', 'Exercise', 'Rest']\n",
        "Prob = np.array([[[1, 0, 0],[.8, .2, 0],[1, 0, 0]], [[.5, .5, 0],[0, 0, 1], [0, 0, 1]], [[.2, .8, 0],[0, 1, 0],[0, 0, 1]]])\n",
        "Reward = np.array([[[20, 0, 0],[40, 30, 0],[100, 0, 0]], [[-10, -10, 0],[0, 0, -10], [0, 0, 0]], [[0, 0, 0],[0, 0, 0], [0, 0, 0]]])\n",
        "\n",
        "vi = mdptoolbox.mdp.ValueIteration(Prob, Reward, 0.86)\n",
        "vi.run()\n",
        "print(vi.policy)\n",
        "print(vi.iter)\n",
        "print(vi.V)\n",
        "\n",
        "print('\\nValue Iteration:')\n",
        "for i in range(3):\n",
        "  a = vi.policy[i]\n",
        "  print('For state', states[i], 'optimal policy', actions[a], 'the optimal values', vi.V[i])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 1, 0)\n",
            "35\n",
            "(162.9774006496814, 196.31488750860825, 240.04128246714959)\n",
            "\n",
            "Value Iteration:\n",
            "For state Exhuasted optimal policy Rest the optimal values 162.9774006496814\n",
            "For state Productive optimal policy Exercise the optimal values 196.31488750860825\n",
            "For state Fit optimal policy Coding the optimal values 240.04128246714959\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBLGwsWuPc1k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "135a71d0-ecee-4dd6-aae5-9e5731108e4e"
      },
      "source": [
        "states = ['Exhuasted', 'Productive', 'Fit']\n",
        "actions = ['Coding', 'Exercise', 'Rest']\n",
        "Prob = np.array([[[1, 0, 0],[.8, .2, 0],[1, 0, 0]], [[.5, .5, 0],[0, 0, 1], [0, 0, 1]], [[.2, .8, 0],[0, 1, 0],[0, 0, 1]]])\n",
        "Reward = np.array([[[20, 0, 0],[40, 30, 0],[100, 0, 0]], [[-10, -10, 0],[0, 0, -10], [0, 0, 0]], [[0, 0, 0],[0, 0, 0], [0, 0, 0]]])\n",
        "\n",
        "pi = mdptoolbox.mdp.PolicyIteration(Prob, Reward, 0.86, policy0  = [0, 0 ,0])\n",
        "pi.run()\n",
        "print(pi.policy)\n",
        "print(pi.iter)\n",
        "print('\\nPolicy Iteration:')\n",
        "for i in range(3):\n",
        "  a = pi.policy[i]\n",
        "  print('For state', states[i], 'optimal policy', actions[a], 'the optimal values', pi.V[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 1, 0)\n",
            "3\n",
            "\n",
            "Policy Iteration:\n",
            "For state Exhuasted optimal policy Rest the optimal values 163.83251784711635\n",
            "For state Productive optimal policy Exercise the optimal values 197.17053019972724\n",
            "For state Fit optimal policy Coding the optimal values 240.89596534852006\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}